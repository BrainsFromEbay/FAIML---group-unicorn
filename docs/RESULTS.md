# Results

The performance of each model in this project is evaluated and presented in a consistent manner. This document explains where to find the results and how to interpret them.

## Location of Results

The results for each model are stored in a `results` subdirectory within the respective contributor's folder. For example:

*   **Jere's CNN:** `Jere/results/`
*   **Oyshe's HOG + Logistic Regression:** `Oyshe/results/`
*   **Mahi's Models:**
    *   `Mahi/results/cnn_pickle/`
    *   `Mahi/results/cnn_raw/`
    *   `Mahi/results/mlp_mnist/`
    *   `Mahi/results/mlp_pickle/`
    *   `Mahi/results/rf_mnist/`

## Content of the Results

Each `results` directory typically contains the following:

*   `confusion_matrix.png`: An image of the confusion matrix generated by testing the model on the custom test set (`custom_test/` directory).
*   `stats.md` (for some models): A Markdown file with additional statistics, such as accuracy, precision, recall, and F1-score.

## Confusion Matrix

A confusion matrix is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are known.

*   **Rows:** The rows of the matrix represent the actual class (the true digit).
*   **Columns:** The columns represent the predicted class (the digit predicted by the model).

The main diagonal of the matrix shows the number of correct predictions for each digit, while the off-diagonal elements show the misclassifications.

The confusion matrices for each model are also displayed in the [Graphical User Interface (GUI)](./GUI.md) when the model is selected.

## Generating the Results

The scripts in the `inference` and `predict` directories often contain code to generate these results. For example, running `Oyshe/prediction.py` will generate the confusion matrix for Oyshe's model.

By comparing the confusion matrices and other statistics, you can get a good understanding of the strengths and weaknesses of each model.
